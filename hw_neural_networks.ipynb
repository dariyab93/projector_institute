{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c82b0-5ec8-4892-85d4-ae826f8bd6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d834be5b-3b40-4523-ad14-a1efa36d524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.5677206656638532\n",
      "Error:0.499999693965172\n",
      "Error:0.49999962187837665\n",
      "Error:0.49999950625248707\n",
      "Error:0.4999992912246485\n",
      "Error:0.4999987559004841\n",
      "Error:0.4999952729191357\n",
      "Error:0.49999999999953304\n",
      "Error:0.49999999999953304\n",
      "Error:0.49999999999953304\n",
      "Training completed in 0.07 seconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import time\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(200, shuffle=False, noise=0.20)\n",
    "HIDDEN_LAYER_SIZE = 10\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "w0 = 2 * np.random.random((2, HIDDEN_LAYER_SIZE)) - 1\n",
    "b0 = np.zeros((1, HIDDEN_LAYER_SIZE))\n",
    "w1 = 2 * np.random.random((HIDDEN_LAYER_SIZE, 1)) - 1\n",
    "b1 = np.zeros((1))\n",
    "\n",
    "# Using tanh activation function\n",
    "def activation(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "for j in range(1000):\n",
    "    # forward propagation\n",
    "    a0 = X\n",
    "    z1 = np.dot(a0, w0) + b0\n",
    "    a1 = activation(z1)\n",
    "    z2 = np.dot(a1, w1) + b1\n",
    "    a2 = activation(z2)\n",
    "\n",
    "    # how much did we miss?\n",
    "    l2_error = a2 - y[:, np.newaxis]\n",
    "\n",
    "    if (j % 100) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "\n",
    "    # back propagation\n",
    "    l2_delta = l2_error * activation(z2, deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * activation(z1, deriv=True)\n",
    "\n",
    "    # update weights\n",
    "    w1 -= LEARNING_RATE * a1.T.dot(l2_delta)\n",
    "    b1 -= LEARNING_RATE * np.sum(l2_delta, axis=0)\n",
    "    w0 -= LEARNING_RATE * a0.T.dot(l1_delta)\n",
    "    b0 -= LEARNING_RATE * np.sum(l1_delta, axis=0)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "254b429a-80d4-416c-9763-ea21e240bff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.7532700351315844\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Training completed in 0.08 seconds.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import time\n",
    "\n",
    "# Create the dataset\n",
    "X, y = sklearn.datasets.make_moons(200, shuffle=False, noise=0.20)\n",
    "HIDDEN_LAYER_SIZE = 10\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# Initialize weights and biases\n",
    "w0 = 2 * np.random.random((2, HIDDEN_LAYER_SIZE)) - 1\n",
    "b0 = np.zeros((1, HIDDEN_LAYER_SIZE))\n",
    "w1 = 2 * np.random.random((HIDDEN_LAYER_SIZE, 1)) - 1\n",
    "b1 = np.zeros((1))\n",
    "\n",
    "# Using ReLU activation function\n",
    "def activation(x, deriv=False):\n",
    "    if deriv:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for j in range(1000):\n",
    "    # Forward propagation\n",
    "    a0 = X\n",
    "    z1 = np.dot(a0, w0) + b0\n",
    "    a1 = activation(z1)\n",
    "    z2 = np.dot(a1, w1) + b1\n",
    "    a2 = activation(z2)\n",
    "\n",
    "    # Calculate error\n",
    "    l2_error = a2 - y[:, np.newaxis]\n",
    "\n",
    "    if (j % 100) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "\n",
    "    # Back propagation\n",
    "    l2_delta = l2_error * activation(z2, deriv=True)\n",
    "\n",
    "    # Calculate contribution to the error from the hidden layer\n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = l1_error * activation(z1, deriv=True)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= LEARNING_RATE * a1.T.dot(l2_delta)\n",
    "    b1 -= LEARNING_RATE * np.sum(l2_delta, axis=0)\n",
    "    w0 -= LEARNING_RATE * a0.T.dot(l1_delta)\n",
    "    b0 -= LEARNING_RATE * np.sum(l1_delta, axis=0)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccf4f6-e5c3-443a-a7d3-8b9c4b0c9a0b",
   "metadata": {},
   "source": [
    "#### The `tanh` function exhibited smoother convergence, suggesting that it better captures the relationships in this dataset. In contrast, the `ReLU` function's quick stabilization at 0.500 indicates potential issues, such as neurons becoming inactive and failing to learn effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf1ad1bb-8720-41fe-9852-136d63c4bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n",
      "Training Error: 0.50625, Validation Error: 0.475\n"
     ]
    }
   ],
   "source": [
    "#adding a hidden layer\n",
    "HIDDEN_LAYER_SIZE_1 = 10\n",
    "HIDDEN_LAYER_SIZE_2 = 10\n",
    "\n",
    "w0 = 2 * np.random.random((2, HIDDEN_LAYER_SIZE_1)) - 1\n",
    "b0 = np.zeros((1, HIDDEN_LAYER_SIZE_1))\n",
    "w1 = 2 * np.random.random((HIDDEN_LAYER_SIZE_1, HIDDEN_LAYER_SIZE_2)) - 1\n",
    "b1 = np.zeros((1, HIDDEN_LAYER_SIZE_2))\n",
    "w2 = 2 * np.random.random((HIDDEN_LAYER_SIZE_2, 1)) - 1\n",
    "b2 = np.zeros((1))\n",
    "\n",
    "# Update the training loop to accommodate an additional layer\n",
    "for j in range(1000):\n",
    "    # Forward propagation\n",
    "    a0 = X_train\n",
    "    z1 = np.dot(a0, w0) + b0\n",
    "    a1 = activation(z1)\n",
    "    z2 = np.dot(a1, w1) + b1\n",
    "    a2 = activation(z2)\n",
    "    z3 = np.dot(a2, w2) + b2\n",
    "    a3 = activation(z3)\n",
    "\n",
    "    # Calculate errors\n",
    "    l2_error = a3 - y_train[:, np.newaxis]\n",
    "    # Validation\n",
    "    val_z1 = np.dot(X_val, w0) + b0\n",
    "    val_a1 = activation(val_z1)\n",
    "    val_z2 = np.dot(val_a1, w1) + b1\n",
    "    val_a2 = activation(val_z2)\n",
    "    val_z3 = np.dot(val_a2, w2) + b2\n",
    "    val_a3 = activation(val_z3)\n",
    "    val_error = val_a3 - y_val[:, np.newaxis]\n",
    "\n",
    "    if (j % 100) == 0:\n",
    "        print(f\"Training Error: {np.mean(np.abs(l2_error))}, Validation Error: {np.mean(np.abs(val_error))}\")\n",
    "\n",
    "    # Back propagation\n",
    "    l2_delta = l2_error * activation(z3, deriv=True)\n",
    "    l1_error = l2_delta.dot(w2.T)\n",
    "    l1_delta = l1_error * activation(z2, deriv=True)\n",
    "    l0_error = l1_delta.dot(w1.T)\n",
    "    l0_delta = l0_error * activation(z1, deriv=True)\n",
    "\n",
    "    # Update weights with L2 regularization\n",
    "    w2 -= LEARNING_RATE * (a2.T.dot(l2_delta) + LAMBDA * w2)\n",
    "    b2 -= LEARNING_RATE * np.sum(l2_delta, axis=0)\n",
    "    w1 -= LEARNING_RATE * (a1.T.dot(l1_delta) + LAMBDA * w1)\n",
    "    b1 -= LEARNING_RATE * np.sum(l1_delta, axis=0)\n",
    "    w0 -= LEARNING_RATE * (a0.T.dot(l0_delta) + LAMBDA * w0)\n",
    "    b0 -= LEARNING_RATE * np.sum(l0_delta, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a422393-2a18-442c-8e2b-66637816018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.7808713512032577, Validation Error: 0.7708634061933959\n",
      "Training Error: 0.2609700025305486, Validation Error: 0.17424982491293817\n",
      "Training Error: 0.27584309338693674, Validation Error: 0.20493776248280296\n",
      "Training Error: 0.22760288509838533, Validation Error: 0.15507067637575933\n",
      "Training Error: 0.21379886191414932, Validation Error: 0.15947931898031145\n",
      "Training Error: 0.22498634709743043, Validation Error: 0.14072389787546566\n",
      "Training Error: 0.20186260435908082, Validation Error: 0.17214475861477457\n",
      "Training Error: 0.23675964798032387, Validation Error: 0.18016989866985494\n",
      "Training Error: 0.2629776401467094, Validation Error: 0.22184550825069982\n",
      "Training Error: 0.21591234612287277, Validation Error: 0.16214890476644098\n",
      "Training completed in 0.12 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "X, y = sklearn.datasets.make_moons(200, shuffle=False, noise=0.20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "HIDDEN_LAYER_SIZE_1 = 10\n",
    "HIDDEN_LAYER_SIZE_2 = 10\n",
    "LEARNING_RATE = 0.01\n",
    "LAMBDA = 0.01  # Regularization strength\n",
    "DROPOUT_RATE = 0.1  # Fraction of neurons to drop\n",
    "\n",
    "# Initialize weights and biases\n",
    "w0 = 2 * np.random.random((2, HIDDEN_LAYER_SIZE_1)) - 1\n",
    "b0 = np.zeros((1, HIDDEN_LAYER_SIZE_1))\n",
    "w1 = 2 * np.random.random((HIDDEN_LAYER_SIZE_1, HIDDEN_LAYER_SIZE_2)) - 1\n",
    "b1 = np.zeros((1, HIDDEN_LAYER_SIZE_2))\n",
    "w2 = 2 * np.random.random((HIDDEN_LAYER_SIZE_2, 1)) - 1\n",
    "b2 = np.zeros((1))\n",
    "\n",
    "# Using tanh activation function\n",
    "def activation(x, deriv=False):\n",
    "    if deriv:\n",
    "        return 1 - np.tanh(x)**2\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Dropout function\n",
    "def dropout(layer, rate):\n",
    "    if rate > 0:\n",
    "        mask = np.random.binomial(1, 1 - rate, size=layer.shape)\n",
    "        return layer * mask, mask  # Return masked layer and the mask\n",
    "    return layer, None\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for j in range(1000):\n",
    "    # Forward propagation\n",
    "    a0 = X_train\n",
    "    z1 = np.dot(a0, w0) + b0\n",
    "    a1 = activation(z1)\n",
    "    a1, mask1 = dropout(a1, DROPOUT_RATE)  # Apply dropout on first hidden layer\n",
    "    z2 = np.dot(a1, w1) + b1\n",
    "    a2 = activation(z2)\n",
    "    a2, mask2 = dropout(a2, DROPOUT_RATE)  # Apply dropout on second hidden layer\n",
    "    z3 = np.dot(a2, w2) + b2\n",
    "    a3 = activation(z3)\n",
    "\n",
    "    # Calculate errors\n",
    "    l2_error = a3 - y_train[:, np.newaxis]\n",
    "\n",
    "    # Validation\n",
    "    val_z1 = np.dot(X_val, w0) + b0\n",
    "    val_a1 = activation(val_z1)\n",
    "    val_z2 = np.dot(val_a1, w1) + b1\n",
    "    val_a2 = activation(val_z2)\n",
    "    val_z3 = np.dot(val_a2, w2) + b2\n",
    "    val_a3 = activation(val_z3)\n",
    "    val_error = val_a3 - y_val[:, np.newaxis]\n",
    "\n",
    "    if (j % 100) == 0:\n",
    "        print(f\"Training Error: {np.mean(np.abs(l2_error))}, Validation Error: {np.mean(np.abs(val_error))}\")\n",
    "\n",
    "    # Back propagation\n",
    "    l2_delta = l2_error * activation(z3, deriv=True)\n",
    "    l1_error = l2_delta.dot(w2.T)\n",
    "    l1_delta = l1_error * activation(z2, deriv=True) * mask2  # Apply dropout mask\n",
    "    l0_error = l1_delta.dot(w1.T)\n",
    "    l0_delta = l0_error * activation(z1, deriv=True) * mask1  # Apply dropout mask\n",
    "\n",
    "    # Update weights with L2 regularization\n",
    "    w2 -= LEARNING_RATE * (a2.T.dot(l2_delta) + LAMBDA * w2)\n",
    "    b2 -= LEARNING_RATE * np.sum(l2_delta, axis=0)\n",
    "    w1 -= LEARNING_RATE * (a1.T.dot(l1_delta) + LAMBDA * w1)\n",
    "    b1 -= LEARNING_RATE * np.sum(l1_delta, axis=0)\n",
    "    w0 -= LEARNING_RATE * (a0.T.dot(l0_delta) + LAMBDA * w0)\n",
    "    b0 -= LEARNING_RATE * np.sum(l0_delta, axis=0)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cccfc-50b4-4043-bd90-3fc457416e89",
   "metadata": {},
   "source": [
    "## Neural Network Performance Summary\n",
    "\n",
    "I conducted experiments on a neural network trained on the \"moons\" dataset using the following configurations:\n",
    "\n",
    "- **Activation Function**: `tanh`\n",
    "- **Dropout Rate**: 0.1 (10% of neurons dropped during training)\n",
    "- **Regularization Strength (Î»)**: 0.01\n",
    "- **Learning Rate**: 0.01\n",
    "\n",
    "### Results\n",
    "\n",
    "- **Training Error**:\n",
    "  - Started at approximately **0.6595** and decreased to **0.1973** over 1000 iterations.\n",
    "  \n",
    "- **Validation Error**:\n",
    "  - Began at around **0.5920** and improved to approximately **0.1172**.\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The model effectively learned from the training data, as indicated by the consistent decrease in both training and validation errors.\n",
    "- The gap between training and validation errors is narrowing, suggesting effective generalization and the positive impact of dropout regularization.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Hyperparameter Tuning**: Experiment with different dropout rates, regularization strengths, and learning rates for further improvements.\n",
    "- **Monitor for Overfitting**: Implement early stopping to prevent overfitting if the validation error increases while training error decreases.\n",
    "- **Evaluation**: Test the final model on a separate dataset to assess performance.\n",
    "\n",
    "Overall, the current configuration is yielding promising results, and further adjustments could enhance the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867425a-787f-465f-ae57-72d5f4e37af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
